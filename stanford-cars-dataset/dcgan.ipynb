{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b76615-b43e-4c8b-a410-311bedca8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8df573-0b91-477d-b760-0610ba5767a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"/media/xiaolong/9bbdf31a-3238-4691-845c-7f9126769abe/Dataset/stanford-cars-dataset/data/cars_train\" \n",
    "IMAGE_DIM = (32, 32, 3)\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90d6c81-0531-4ea8-8ff8-7daa5def5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channel=1, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 28 -> 14\n",
    "            nn.Conv2d(in_channel, 512, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 14 -> 7\n",
    "            nn.Conv2d(512, 256, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 7-> 4\n",
    "            nn.Conv2d(256, 128, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #\n",
    "            nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            # reshape input, 128 -> 1\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc3b351-f595-4087-8b37-9e1351e934b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, out_channel=1, input_size=100, num_classes=784):\n",
    "        super(Generator, self).__init__()\n",
    "        assert IMAGE_DIM[0] % 2**4 == 0, \"Should be divided 16\"\n",
    "        self.init_dim = (IMAGE_DIM[0] // 2**4, IMAGE_DIM[1] // 2**4)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, self.init_dim[0] * self.init_dim[1] * 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # x2\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # x2\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # x2\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # x2\n",
    "            nn.ConvTranspose2d(128, out_channel, 4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 512, self.init_dim[0], self.init_dim[1])\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8271323-6435-455e-bc6c-ac98637a1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(in_channel=IMAGE_DIM[-1]).to(DEVICE)\n",
    "G = Generator(out_channel=IMAGE_DIM[-1]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1565a9c-aacb-4fff-a5a2-8a1f6b3b0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARDataset(Dataset):\n",
    "    def __init__(self, data_path, tansform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.fpaths = sorted(glob.glob(os.path.join(data_path, \"*/*.jpg\")))\n",
    "        \n",
    "        gray_lst = [266, 1085, 2176, 3048, 3439, 3469, 3539, 4577, 4848,\n",
    "                   5177, 5502, 5713, 6947, 7383, 7693, 7774, 8137, 8144]\n",
    "        for fpath in self.fpaths:\n",
    "            if any([str(i) in fpath for i in gray_lst]):\n",
    "                self.fpaths.remove(fpath)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(Image.open(self.fpaths[idx]))\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76eec76d-0d4c-45bb-9042-8d70a9f2466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_DIM[0], IMAGE_DIM[1])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = CARDataset(DATA_PATH, transform)\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7854d2-305a-4508-9342-ebae6635d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "G_opt = torch.optim.Adam(G.parameters(), lr=0.001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888f51eb-920a-4a58-8446-128bf114c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "EPOCHS = 10\n",
    "N_CRITIC = 1 # for training more k steps about Discriminator\n",
    "N_NOISE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77850f82-b6a3-48b5-a302-d2f6227c3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_labels = torch.ones([BATCH_SIZE, 1]).to(DEVICE) # Discriminator Label to real\n",
    "D_fakes = torch.zeros([BATCH_SIZE, 1]).to(DEVICE) # Discriminator Label to fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bcc36a8-040c-448b-b86f-1fbb22aace67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, n_noise):\n",
    "    z = torch.randn(10, n_noise).to(DEVICE)\n",
    "    y_hat = G(z).view(10, 3, 32, 32).permute(0, 2, 3, 1)\n",
    "    result = (y_hat.detach().cpu().numpy() + 1) / 2.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b3db9f-d7df-46e6-a8dc-e4a2ba0fa225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/126 [00:01<00:58,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Step: 0, D Loss: 1.3978, G Loss: 0.7397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:26<00:00,  4.79it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.83it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.81it/s]\n",
      " 98%|█████████▊| 124/126 [00:25<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10, Step: 500, D Loss: 0.7252, G Loss: 2.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:26<00:00,  4.81it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.81it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.81it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.77it/s]\n",
      " 95%|█████████▌| 120/126 [00:24<00:01,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10, Step: 1000, D Loss: 0.9676, G Loss: 1.8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:26<00:00,  4.80it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.75it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for images in tqdm(data_loader):\n",
    "        # Training Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        x_outputs = D(x)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "        \n",
    "        z = torch.randn(BATCH_SIZE, N_NOISE).to(DEVICE)\n",
    "        G(z)\n",
    "        z_outputs = D(G(z))\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "        \n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        if step % N_CRITIC == 0:\n",
    "            # Training Generator\n",
    "            z = torch.randn(BATCH_SIZE, N_NOISE).to(DEVICE)\n",
    "            z_outputs = D(G(z))\n",
    "            G_loss = criterion(z_outputs, D_labels)\n",
    "            \n",
    "            D.zero_grad()\n",
    "            G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_opt.step()\n",
    "            \n",
    "        if step % 500 == 0:\n",
    "            print(\"Epoch: {}/{}, Step: {}, D Loss: {:.4f}, G Loss: {:.4f}\".format(\n",
    "                epoch, EPOCHS, step, D_loss.item(), G_loss.item()\n",
    "            ))\n",
    "            G.eval()\n",
    "            img = get_sample_image(G, N_NOISE)\n",
    "            plt.imsave(\"samples/dcgan_step{:05d}.jpg\".format(step), img[0])\n",
    "            G.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed5fc462-c4e8-47f7-aa5a-67ff8d5546f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41756239d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generation to image\n",
    "G.eval()\n",
    "plt.imshow(get_sample_image(G, N_NOISE)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc962e8-3005-41ae-b176-4d39b22ca967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name='checkpoint.pth'):\n",
    "    torch.save(state, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8576fe09-3817-4311-9392-5522562089f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving params\n",
    "# torch.save(D.state_dict(), 'D_c.pkl')\n",
    "# torch.save(G.state_dict(), 'G_c.pkl')\n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'D':D.state_dict(),\n",
    "                 'G':G.state_dict(),\n",
    "                 'd_optim': D_opt.state_dict(),\n",
    "                 'g_optim' : G_opt.state_dict()},\n",
    "            'dcgan.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bf2ba-1738-46d9-9a5f-4a401390ec1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88715739-1b27-467e-944d-781eb8bd710a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
